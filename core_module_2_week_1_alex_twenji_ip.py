# -*- coding: utf-8 -*-
"""Core_Module_2_Week_1_Alex_Twenji_IP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yxdLxjca04ihYujkG769qlI3rJVneXNQ

# DEFINING THE QUESTION

## a) Specifying the Question

You have been recruited as a football analyst in a company - Mchezopesa Ltd and tasked to accomplish the task below.

A prediction result of a game between team 1 and team 2, based on who's home and who's away, and on whether or not the game is friendly (include rank in your training).

## b) Defining the Metric for Success

## c) Understanding the context

The men's FIFA World Ranking is a ranking system for men's national teams in association football, currently led by Belgium (as of December 2020). The teams of the men's member nations of FIFA, football's world governing body, are ranked based on their game results with the most successful teams being ranked highest. The rankings were introduced in December 1992, and eight teams (Argentina, Belgium, Brazil, France, Germany, Italy, the Netherlands and Spain) have held the top position, of which Brazil have spent the longest ranked first.

A points system is used, with points being awarded based on the results of all FIFA-recognised full international matches.

The ranking system has been revamped on several occasions, generally responding to criticism that the preceding calculation method did not effectively reflect the relative strengths of the national teams. The current version of the ranking system was first used on 16 August 2018, adapted from the Elo rating system used in chess and Go.

Membership of FIFA has expanded from 167 to 211 since the rankings began; 210 members are currently included in the rankings. The Cook Islands are the sole unranked FIFA member association, having been removed from the ranking in September 2019 after not playing any fixtures in the previous four years.

The rankings are used by FIFA to rank the progression and current ability of the national football teams of its member nations, and claims that they create "a reliable measure for comparing national A-teams". They are used as part of the calculation, or the entire grounds to seed competitions. In the 2010 FIFA World Cup qualification tournament, the rankings were used to seed the groups in the competitions involving CONCACAF members (using the May rankings), CAF (with the July set of data), and UEFA, using the specially postponed November 2007 ranking positions.

The October 2009 ranking was used to determine the seeds for the 2010 FIFA World Cup final draw. The March 2011 ranking was used to seed the draw for the 2012 CAF Men's Pre-Olympic Tournament second qualifying round.

The rankings are also used to determine the winners of the two annual awards national teams receive on the basis of their performance in the rankings.

The (English) Football Association uses the average of the last 24 months of rankings as one of the criteria for player work permits.

## d) Experimental Design

1. Perform EDA
2. Check Multicollinearity
3. Build Polyniamial Regression Model
4. Cross-validate The Polyniamial Regression Model
5. Create residual plots for your models, and assess their heteroscedasticity using Bartlettâ€™s test
6. Perform any necessary Feature Engineering
7. Build a Logistic Regression Model
8. Perform hyperparameter tuning for the Logistic Regression Model.

# DATA PREPARATION
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

Ranking = pd.read_csv('/content/fifa_ranking.csv')
Ranking

Results = pd.read_csv('/content/results.csv')
Results

Ranking.shape , Results.shape

Ranking.head()

Results.head()

Ranking.tail()

Results.tail()

Ranking.info()

Results.info()

"""# DATA CLEANING"""

Ranking.isna().sum()

Ranking.duplicated().sum()

Ranking[Ranking.duplicated()]

# The duplicated data is all from Sudan and upon further investigation, it all looks fine.

Results.duplicated().sum()

# We have to merge the 2 datasets. from the Polynomial assessement requirements, the model should check for
# Home team Rank, Away team and Tournament type to Predict:
# 1. how many goals the home team scores.
# 2. how many goals the away team scores.
# We therefore need the Rank column from the Rankings dataset to merge onto the results dataset

Rankings_only = Ranking.drop(columns=['country_abrv', 'total_points', 'previous_points', 'rank_change',
                                       'cur_year_avg','cur_year_avg_weighted', 'last_year_avg',	'last_year_avg_weighted',
                                       'two_year_ago_avg',	'two_year_ago_weighted',	'three_year_ago_avg',	'three_year_ago_weighted',
                                       'confederation'])
Rankings_only

import datetime

# Converting Object type dates to datetime format for ease of manipulation

Rankings_only.rank_date = pd.to_datetime(Rankings_only.rank_date)

# Getting the Year of the rankings and seeing the last year of rankings available is 2018
Rankings_only['year'] = Rankings_only['rank_date'].dt.year
Rankings_only.tail(10)

Results.head()

# Converting Object type dates to datetime format for ease of manipulation

Results.date = pd.to_datetime(Results.date)

# Getting the Year of the rankings and seeing the last year of rankings available is 2019
Results['year'] = Results['date'].dt.year
Results.tail(10)

# Since we want to work with rankings data as well, and we don't have rankings for 2019, we will drop data from 2019.
Results_target = Results[Results.year < 2019]
Results_target.tail()

# Also, according to Fifa, ranking depends on results from the previous 4 years, therefore, since our data's latest
# year of ranking is 2018, we'll focus on data from 2015

Results_target = Results_target[Results_target.year > 2014]
Results_target.head(10)

# Merging the 2 datasets to get ranking data for home team

results_merged = Results_target.merge(Rankings_only, left_on=['home_team', 'year'], right_on=['country_full', 'year'], how='inner')
results_merged

results_merged = results_merged.merge(Rankings_only, left_on=['away_team', 'year'], right_on=['country_full', 'year'], how='inner')
results_merged

# Let's drop unneccesary columns and remove the anomalies of different dates and rankings repeating themselves on the same match.

results_merged = results_merged.drop(columns=['city', 'country', 'neutral', 'year', 'country_full_x', 'country_full_y'])

results_merged = results_merged[results_merged.rank_date_x == results_merged.rank_date_y]
results_merged

results_merged.duplicated(subset=['date','home_team','away_team','home_score','away_score','tournament']).sum()

# The above shows that the same match has different rankings duplicated on it. To get rid of this, we'll keep the 
# first row of data of each match only, as this is the one that corresponds to the true rankings of the teams, as the
# succeeding rankings are iterations of the away team rankings over the home team rankings.

results_merged.drop_duplicates(subset=['date','home_team','away_team','home_score','away_score','tournament'], keep= 'first', inplace= True)

results_merged

# we can now drop the remaining unrequired data and rename the columns
results_merged = results_merged.drop(columns=['rank_date_x', 'rank_date_y'])

results_merged.rename(columns={'rank_x':'home_team_rank', 'rank_y':'away_team_rank'},inplace=True)
results_merged

# We now have to change the tournament type to binary numbers.

results_merged.tournament.unique()

results_merged.tournament.nunique()

# According to Fifa, there are 5 categories of games i.e. Friendlies, Regional Competitions,
# Confederations Cup (Pre-cursor to the World Cup), World Cup Qualifiers and The World Cup itself.
# We'll therefore reduce our data to these 5 categories.

def Tourna(Match):
  if Match == Match == 'FIFA World Cup':
    return 5
  elif Match == 'FIFA World Cup qualification':
    return 4
  elif Match == 'Confederations Cup':
    return 3
  elif Match == 'Friendly':
    return 1
  else:
    return 2

# Applying the above function 

results_merged['competition'] = results_merged['tournament'].apply(lambda x: Tourna(x))
results_merged['competition'].unique()

"""# DATA ANALYSIS

## Exploratory Data Aanalysis
"""

results_merged.info()

col_names = ['home_score','away_score', 'home_team_rank', 'away_team_rank', 'competition']

fig, ax = plt.subplots(len(col_names), figsize= (8,40))

for i, col_val in enumerate(col_names):
  sns.boxplot(y = results_merged[col_val], ax= ax[i])
  ax[i].set_title('Box plot - {}'.format(col_val), fontsize= 10)
  ax[i].set_xlabel(col_val, fontsize= 8)
plt.show()

"""The outliers could be due to high scoring matches that are a rarety, but still happen, e.g. the 7-0 drumming of Brazil by Germany at the semi-finals of the 2014 World Cup, we'll therefore keep them.

## SUMMARY STATS
"""

results_merged.describe()

"""# MODELING

## A) POLYNOMIAL REGRESSION MODEL

### Model 1: Checking Multicolinearity
"""

results_merged.head()

# Model 1: Predict how many goals the home team scores. dependent variable here is home_score

independent_home_goals = results_merged.drop(columns=['date', 'home_team', 'away_team', 'home_score', 'tournament'])
correlations_home_goals = independent_home_goals.corr()
correlations_home_goals

# Let's use these correlations to compute the VIF score for each variable.
pd.DataFrame(np.linalg.inv(correlations_home_goals.values), index = correlations_home_goals.index, 
             columns=correlations_home_goals.columns)

"""The VIF scores of all columns are way below 5, indicating we have minimal multicolinearity in the data.

### Model 1: Building the Model
"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

from sklearn.model_selection import train_test_split

X = independent_home_goals.values
y = results_merged['home_score'].values

# Split the dataset into train and test sets
X_train, X_test, y_train,  y_test = train_test_split(X,y, test_size = 0.2, random_state=0)

# Fit polynomial Regression to the dataset
poly_reg = PolynomialFeatures(degree = 2) 
X_poly = poly_reg.fit_transform(X)


pol_reg = LinearRegression()
pol_reg.fit(X_poly, y)

y_pred = pol_reg.predict(poly_reg.fit_transform(X_test))

# let's check with 3 degrees of freedom

poly_reg_3 = PolynomialFeatures(degree = 3) 
X_poly_3 = poly_reg_3.fit_transform(X)

pol_3_reg = LinearRegression()
pol_3_reg.fit(X_poly_3, y)

y_pred_3 = pol_3_reg.predict(poly_reg_3.fit_transform(X_test))

# Now let's try with 4 degrees of freedom

poly_reg_4 = PolynomialFeatures(degree = 4) 
X_poly_4 = poly_reg_4.fit_transform(X)

pol_4_reg = LinearRegression()
pol_4_reg.fit(X_poly_4, y)

y_pred_4 = pol_4_reg.predict(poly_reg_4.fit_transform(X_test))

from sklearn import metrics

print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_3)))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_4)))

# Since we have 4 features, we can use 4 degrees of freedom to avoid over or underfitting the model,
# since it has the lowest RMSE.

"""### Model 1: Cross Validating the Model"""

from sklearn.model_selection import KFold

folds = KFold(n_splits=5)
print('we are using ' +str(folds.get_n_splits(X)) + ' folds')

RMSES = [] # We will use this array to keep track of the RSME of each model
count = 1 # This will just help 
for train_index, test_index in folds.split(X):
  print('\nTraining model ' + str(count))
  
  # set up the train and test based on the split determined by KFold
  # With 5 folds, we will end up with 80% of our data in the training set, and 20% in the test set, just as above
  Xc_train, Xc_test = X[train_index], X[test_index]
  yc_train, yc_test = y[train_index], y[test_index]
  
  # fit a model accordingly
  regressor = LinearRegression()  
  regressor.fit(Xc_train, yc_train)
  
  # assess the accuraccy of the model
  yc_pred = regressor.predict(Xc_test)
  
  rmse_value =  np.sqrt(metrics.mean_squared_error(yc_test, yc_pred))
  RMSES.append(rmse_value)
  
  print('Model ' + str(count) + ' Root Mean Squared Error:',rmse_value)
  count = count + 1

# The means of the RMSEs are within the range of those we found in the Polynomial Regression Model we built.

np.mean(RMSES)

"""As you can see, the average is fairly close to our initial value. However in this case, it might be worthwile to pick model 3 instead, as it has the least RMSE, and is closest to our polynomial regression model with a degree of freedom of 4.

### Model 1: Residual Plots and heteroscedasticity using Bartlettâ€™s test
"""

residuals_home_score = np.subtract(y_pred_4, y_test)

pd.DataFrame(residuals_home_score).describe()

import matplotlib.pyplot as plt

plt.scatter(y_pred_4, residuals_home_score, color='black')
plt.ylabel('residual')
plt.xlabel('fitted values')
plt.axhline(y= residuals_home_score.mean(), color='red', linewidth=1)
plt.show()

# The residuals are centered around a mean that is very close to 0, and there are no glaringly obvious patterns. 
# This shows that the model is fairly good

# Let's be thorough though, and perform a heteroskedasticity test.
# For this we will use bartlett's test. The test establishes as a null hypothesis that the variance is equal for all our datapoints,
# and the new hypothesis that the variance is different for at least one pair of datapoints.

import scipy as sp

test_result, p_value = sp.stats.bartlett(y_pred_4, residuals_home_score)

# To interpret the results we must also compute a critical value of the chi squared distribution
degree_of_freedom = len(y_pred_4)-1
probability = 1 - p_value

critical_value = sp.stats.chi2.ppf(probability, degree_of_freedom)
print(critical_value)

if (test_result > critical_value):
  print('the variances are unequal, and the model should be reassessed')
else:
  print('the variances are homogeneous!')

"""Model 1 has proven sufficient for predicting the home team goals and after cross validation and checking the residuals, whose variances are homogenous, we can be comfortable with our results.

### Model 2: Checking Multicolinearity
"""

# Model 2: Predict how many goals the home team scores. dependent variable here is away_score

independent_away_goals = results_merged.drop(columns=['date', 'home_team', 'away_team', 'away_score', 'tournament'])
correlations_away_goals = independent_away_goals.corr()
correlations_away_goals

# Let's use these correlations to compute the VIF score for each variable.
pd.DataFrame(np.linalg.inv(correlations_away_goals.values), index = correlations_away_goals.index, 
             columns=correlations_away_goals.columns)

"""The VIF scores of all columns are also way below 5, indicating we have minimal multicolinearity in the data.

### Model 2: Building the Model
"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

from sklearn.model_selection import train_test_split

A = independent_away_goals.values
b = results_merged['away_score'].values

# Split the dataset into train and test sets
A_train, A_test, b_train, b_test = train_test_split(A,b, test_size = 0.2, random_state=0)

# Fit polynomial Regression to the dataset
poly_reg = PolynomialFeatures(degree = 2) 
A_poly = poly_reg.fit_transform(A)

pol_reg = LinearRegression()
pol_reg.fit(A_poly, b)

b_pred = pol_reg.predict(poly_reg.fit_transform(A_test))

poly_reg_3 = PolynomialFeatures(degree = 3) 
A_poly_3 = poly_reg_3.fit_transform(A)

pol_3_reg = LinearRegression()
pol_3_reg.fit(A_poly_3, b)

b_pred_3 = pol_3_reg.predict(poly_reg_3.fit_transform(A_test))

poly_reg_4 = PolynomialFeatures(degree = 4) 
A_poly_4 = poly_reg_4.fit_transform(A)

pol_4_reg = LinearRegression()
pol_4_reg.fit(A_poly_4, b)

b_pred_4 = pol_4_reg.predict(poly_reg_4.fit_transform(A_test))

print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(b_test, b_pred)))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(b_test, b_pred_3)))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(b_test, b_pred_4)))

# Since we have 4 features, we can use 4 degrees of freedom to avoid over or underfitting the model,
# since it has the lowest RMSE.

"""### Model 2: Cross Validating the Model"""

folds = KFold(n_splits=5)
print('we are using ' +str(folds.get_n_splits(A)) + ' folds')

RMSES = [] # We will use this array to keep track of the RSME of each model
count = 1 # This will just help 
for train_index, test_index in folds.split(A):
  print('\nTraining model ' + str(count))
  
  # set up the train and test based on the split determined by KFold
  # With 5 folds, we will end up with 80% of our data in the training set, and 20% in the test set, just as above
  Ac_train, Ac_test = A[train_index], A[test_index]
  bc_train, bc_test = b[train_index], b[test_index]
  
  # fit a model accordingly
  regressor = LinearRegression()  
  regressor.fit(Ac_train, bc_train)
  
  # assess the accuraccy of the model
  bc_pred = regressor.predict(Ac_test)
  
  rmse_value =  np.sqrt(metrics.mean_squared_error(bc_test, bc_pred))
  RMSES.append(rmse_value)
  
  print('Model ' + str(count) + ' Root Mean Squared Error:',rmse_value)
  count = count + 1

# The means of the RMSEs are within the range of those we found in the Polynomial Regression Model we built.

np.mean(RMSES)

"""As you can see, the average is fairly close to our initial value. However in this case, it might be worthwile to pick model 2 instead, as it has the least RMSE, and is closest to our polynomial regression model with a degree of freedom of 4.

### Model 2: Residual Plots and heteroscedasticity using Bartlettâ€™s test
"""

residuals_away_score = np.subtract(b_pred_4, b_test)

pd.DataFrame(residuals_away_score).describe()

plt.scatter(b_pred_4, residuals_away_score, color='black')
plt.ylabel('residual')
plt.xlabel('fitted values')
plt.axhline(y= residuals_away_score.mean(), color='red', linewidth=1)
plt.show()

# The residuals are centered around a mean that is very close to 0, and there are no glaringly obvious patterns. 
# This shows that the model is fairly good

# Let's be thorough though, and perform a heteroskedasticity test.
# For this we will use bartlett's test. The test establishes as a null hypothesis that the variance is equal for all our datapoints,
# and the new hypothesis that the variance is different for at least one pair of datapoints.

import scipy as sp

test_result, p_value = sp.stats.bartlett(b_pred_4, residuals_away_score)

# To interpret the results we must also compute a critical value of the chi squared distribution
degree_of_freedom = len(b_pred_4)-1
probability = 1 - p_value

critical_value = sp.stats.chi2.ppf(probability, degree_of_freedom)
print(critical_value)

if (test_result > critical_value):
  print('the variances are unequal, and the model should be reassessed')
else:
  print('the variances are homogeneous!')

"""Model 2 has proven sufficient for predicting the home team goals and after cross validation and checking the residuals, whose variances are homogenous, we can be comfortable with our results.

## B) LOGISTIC REGRESSION MODEL

### Feature Engineering
"""

results_merged.head()

# Feature Engineering: Figure out from the home teamâ€™s perspective if the game is a Win, Lose or Draw (W, L, D)

def match_result(row):
  if row['home_score'] > row['away_score']:
    outcome = 'Win'
  elif row['home_score'] < row['away_score']:
    outcome = 'Lose'
  else:
    outcome = 'Draw'
  return outcome

results_merged['result'] = results_merged.apply(match_result, axis=1)
results_merged

from sklearn.preprocessing import LabelEncoder

labelencoder = LabelEncoder()
results_merged['result'] = labelencoder.fit_transform(results_merged['result'])
results_merged

"""### Building the Model"""

C = results_merged.drop(columns= ['date',	'home_team',	'away_team', 'tournament', 'result'])
d = results_merged['result']

from sklearn.model_selection import train_test_split
C_train, C_test, d_train, d_test = train_test_split(C, d, test_size = .2, random_state=20)

from sklearn.linear_model import LogisticRegression

LogReg = LogisticRegression()
LogReg.fit(C_train, d_train)

d_pred = LogReg.predict(C_test)

# Evaluating the model
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(d_test, d_pred)
confusion_matrix

"""The results from the confusion matrix tell us that 136, 167 and 286 are the number of correct predictions. There seems to be no incorrect predictions.

### Hyperparameter Testing
"""

# Alternative Solution

# scaling data as advised by the warning after running the previous cell.
from sklearn import linear_model
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(C_train, d_train)
C_train = scaler.transform(C_train)

# Creating the logistic regression to improve the regression as suggested by the warning
# after running the previous cell.

logistic = linear_model.LogisticRegression()

# Creating regularization penalty space
penalty = ['l1', 'l2']

# Creating regularization hyperparameter space
hyp_C = np.logspace(0, 4, 10)

solver = [ 'liblinear', 'sag', 'saga']

# Creating hyperparameter options
hyperparameters = dict(C=hyp_C, penalty=penalty, solver = solver, max_iter = (10,100))

# Creating grid search using 5-fold cross validation
clf = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)

# Fitting grid search
best_model = clf.fit(C_train, d_train)

# Viewing best hyperparameters
print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])
print('Best C:', best_model.best_estimator_.get_params()['C'])
print('Best Solver:', best_model.best_estimator_.get_params()['solver'])
print('Best max_iter:', best_model.best_estimator_.get_params()['max_iter'])

# Predicting target vector
best_model.predict(C)

# Creating the logistic regression
logistic = linear_model.LogisticRegression().fit(X_train,y_train)
metrics.accuracy_score(d_test, d_pred)

logistic = linear_model.LogisticRegression(penalty='l1', C=1, max_iter=10, solver='saga').fit(C_train,d_train)

dc_pred = logistic.predict(C_test)
metrics.accuracy_score(d_test, dc_pred)

best_model.best_score_

"""The above model achieves 100% accuracy. the best parameters are C is 1 and L1 penalty, yto avoid overfitting the model

# CONCLUSION

Both Polynomial amd Logistic regression were fairly accurate in predicting the results, however, since the Logistic regression was classifying the results into either win,lose or draw, it provided more accuracy at 100 %. Continuous variables are generally ,are difficult to predict to the exact value, however, the variance in our data is acceptable.

## FOLLOW UP QUESTIONS

### a.) Did we have the right data?

Somewhat Yes we did, although merging it was an issue due to the iteration of ranks over the same match. We however solved this by eliminating the unnecessary duplicates.

### b.) Do we need other data to answer our question?

Somewhat Yes, it could be improved by having player data and player performance data, as star plays like Paul Pogba, Lionel Messi and Cristiano Ronaldo could single handedly win games for their teams. Player performance data could help improve the predictions.

### c.) Did we have the right question?

Yes we did. This is an interesting challenge as the ability for a team to win helps sports betting companies create their betting odds.
"""

